# -*- coding: utf-8 -*-
"""TD3 TF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SyuGvZPxotuH37AMBhf9omTY5GI6nn2N
"""

!pip install pybullet

import tensorflow as tf
import numpy as np
import os
import time
import random
import numpy as np
import matplotlib.pyplot as plt
import pybullet_envs
import gym

class Critic(tf.keras.Model):
  def __init__(self,action_dim, state_dim):
    super(Critic,self).__init__()
    self.actor_input = tf.keras.layers.Dense(state_dim + action_dim, activation='relu')
    self.c1 = tf.keras.layers.Dense(400,activation='relu')
    self.c2 = tf.keras.layers.Dense(300,activation='relu')
    self.v = tf.keras.layers.Dense(1,activation=None)

  def forward(self,input_state, action):
    x = self.actor_input(tf.concat([input_state,action],axis = 1))
    x = self.c1(x)
    x = self.c2(x)
    x = self.v(x)
    return x

class Actor(tf.keras.Model):
  def __init__(self, action_dim, state_dim):
    super(Actor,self).__init__()
    self.critic_input = tf.keras.layers.Dense(state_dim, activation='relu')
    self.c1 = tf.keras.layers.Dense(400,activation='relu')
    self.c2 = tf.keras.layers.Dense(300,activation='relu')
    self.policy = tf.keras.layers.Dense(action_dim,activation='tanh')
  
  def forward(self, input_state):
    x = self.critic_input(input_state)
    x = self.c1(x)
    x = self.c2(x)
    x = self.policy(x)
    return x

class ReplayBuffer(object):
  def __init__(self, max_size=1e6):
    self.storage = []
    self.max_size = max_size
    self.ptr = 0

  def add(self,transition):
    if len(self.storage)==self.max_size:
      self.storage[int(self.ptr)] = transition
      self.ptr = (self.ptr+1)%self.max_size
    else:
      self.storage.append(transition)

  def sample(self,batch_size=100):
    ind = np.random.randint(0,len(self.storage),size = batch_size)
    batch_states, batch_next_states, batch_rewards,batch_actions, batch_dones = [],[],[],[],[]
    for i in ind:
      state,next_state,reward,action,done = self.storage[i]
      batch_states.append(np.array(state,copy = False))
      batch_next_states.append(np.array(next_state,copy = False))
      batch_actions.append(np.array(action,copy = False))
      batch_rewards.append(np.array(reward,copy = False))
      batch_dones.append(np.array(done,copy = False))
    return np.array(batch_states),np.array(batch_next_states),np.array(batch_rewards).reshape(-1,1),np.array(batch_actions),np.array(batch_dones).reshape(-1,1)

class Agent():
  def __init__(self,action_dim, state_dim, max_iterations, action_min, action_max, 
               warm_up = 10000, batch_size=100, learning_rate=0.001, memory_size=100000, tau = 0.005,
               policy_noise = 0.1, discount = 0.99, actor_update_steps=2):
    self.actor = Actor(action_dim, state_dim)
    self.actor_target = Actor(action_dim, state_dim)

    for t, e in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):
      t.assign(e)

    self.critic_1 = Critic(action_dim, state_dim)
    self.critic_2 = Critic(action_dim, state_dim)
    self.critic_1_target = Critic(action_dim, state_dim)
    self.critic_2_target = Critic(action_dim, state_dim)

    for t, e in zip(self.critic_1_target.trainable_variables,
                    self.critic_2_target.trainable_variables,
                    self.critic_1.trainable_variables,
                    self.critic_2.trainable_variables):
      t.assign(e)
    
    self.action_dim = action_dim
    self.state_dim = state_dim

    self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    self.critic_1_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    self.critic_2_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    self.memory = ReplayBuffer(memory_size)

    self.actor_target.compile(optimizer = self.actor_optimizer)
    self.critic_1_target.compile(optimizer = self.critic_1_optimizer)
    self.critic_1_target.compile(optimizer = self.critic_2_optimizer)

    self.max_iterations = max_iterations
    self.warm_up = warm_up
    self.current_iteration = 0

    self.policy_noise = policy_noise

    self.discount = discount

    self.batch_size = batch_size

    self.actor_update_steps = 2

    self.tau = tau

    self.action_min = action_min
    self.action_max = action_max

  
  def act(self, state, evaluate = False):
    # if self.current_iteration > self.warm_up:
    #         evaluate = True
    state = tf.convert_to_tensor([state], dtype=tf.float32)
    action = self.actor.forward(state)
    if not evaluate:
      action += tf.random.normal(shape=[self.action_dim], mean=0.0, stddev= self.policy_noise)
    
    action = self.action_max * (tf.clip_by_value(action, self.action_min, self.action_max))
    return action[0]

  def update_target(self):
    actor_target_new_weights = []
    actor_targets_weights = self.actor_target.weights
    for i, weight in enumerate(self.actor.weights):
        actor_target_new_weights.append(weight * self.tau + actor_targets_weights[i]*(1-self.tau))
    self.actor_target.set_weights(actor_target_new_weights)

    critic_1_target_new_weights = []
    critic_1_target_weights = self.critic_1_target.weights
    for i, weight in enumerate(self.critic_1.weights):
      critic_1_target_new_weights.append(weight * self.tau + critic_1_target_weights[i]*(1-self.tau))
    self.critic_1_target.set_weights(critic_1_target_new_weights)

    critic_2_target_new_weights = []
    critic_2_target_weights = self.critic_2_target.weights
    for i, weight in enumerate(self.critic_2.weights):
      critic_2_target_new_weights.append(weight * self.tau + critic_2_target_weights[i]*(1-self.tau))
    self.critic_2_target.set_weights(critic_2_target_new_weights)

    
  def train(self):
    if len(self.memory.storage) < self.batch_size:
      return

    states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)
    # print(states.shape)
    # print(actions.shape)

    states = tf.convert_to_tensor(states, dtype= tf.float32)
    next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)
    rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)
    actions = tf.convert_to_tensor(actions, dtype= tf.float32)
    #dones = tf.convert_to_tensor(dones, dtype= tf.bool)

    with tf.GradientTape() as GT1, tf.GradientTape() as GT2:
      target_actions = self.actor_target.forward(next_states)
      target_actions += tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev= self.policy_noise)
      target_actions = self.action_max * (tf.clip_by_value(target_actions, self.action_min, self.action_max))

      target_v1 = tf.squeeze(self.critic_1_target.forward(next_states,target_actions),1)
      target_v2 = tf.squeeze(self.critic_2_target.forward(next_states,target_actions),1)

      target_v = tf.math.minimum(target_v1,target_v2)
      # print(target_v.shape)
      # print(rewards.shape)
      # # print(self.discount.shape)
      # print(dones.shape)
      target_v = rewards + self.discount * target_v * (1-dones)

      critic_v1 = tf.squeeze(self.critic_1.forward(states,actions))
      critic_v2 = tf.squeeze(self.critic_2.forward(states,actions))

      critic_loss_1 = tf.keras.losses.MSE(target_v,critic_v1)
      critic_loss_2 = tf.keras.losses.MSE(target_v,critic_v2)

    Grad_1 = GT1.gradient(critic_loss_1,self.critic_1.trainable_variables)
    Grad_2 = GT2.gradient(critic_loss_2,self.critic_2.trainable_variables)

    self.critic_1_optimizer.apply_gradients(zip(Grad_1,self.critic_1.trainable_variables))
    self.critic_2_optimizer.apply_gradients(zip(Grad_2,self.critic_2.trainable_variables))

    self.current_iteration+=1
    # print(self.current_iteration)
    if self.current_iteration%self.actor_update_steps == 0:
      
      with tf.GradientTape() as GT:
        # print(states.shape)
        policies = self.actor.forward(states)
        actor_loss = tf.math.reduce_mean(-self.critic_1.forward(states,policies))

      Grad_actor = GT.gradient(actor_loss,self.actor.trainable_variables)
      self.actor_optimizer.apply_gradients(zip(Grad_actor,self.actor.trainable_variables))

      self.update_target()

def evaluate_policy(agent, eval_period=10):
  avg_reward = 0
  for _ in range(eval_period):
    obs=env.reset()
    done=False
    while not done:
      action = agent.act(obs)
      obs,reward,done,_ = env.step(action)
      avg_reward += reward
    avg_reward = avg_reward/eval_period
    print('Average reward over the Evaluation step %f'% (avg_reward))
    return avg_reward

env_name = 'HalfCheetahBulletEnv-v0'
env= gym.make(env_name)

tf.random.set_seed(336699)
# action_dim, max_iterations = 2, 100000
# action_min, action_max = 0.0,1.0

max_iterations = 5e5
warm_up = 10000
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
action_max = float(env.action_space.high[0])
action_min = -float(env.action_space.high[0])

eval_freq  = 5e3

agent = Agent(action_dim, state_dim, max_iterations,action_min,action_max)
ep_reward = []
total_avgr = []
target = False
evaluations = [evaluate_policy(agent)]
print(evaluations)
# env = Environment()

s = 0
total_reward = 0 
episode_timesteps = 0 
state = env.reset()
done = False
while s<=max_iterations:

  if s<warm_up:
    action = env.action_space.sample()
  else:
    action = agent.act(state)

  next_state, reward, done, _ = env.step(action)

  done_bool = 0 if episode_timesteps + 1 == env.spec.max_episode_steps else float(done)

  agent.memory.add((state, next_state, reward, np.array(action), done_bool))
  agent.train()

  state = next_state
  total_reward += reward

  episode_timesteps +=1

  if done:
    print('episode')
    ep_reward.append(total_reward)
    avg_reward = np.mean(ep_reward[-100:])
    total_avgr.append(avg_reward)
    total_reward = 0
    episode_timesteps = 0
    state = env.reset()

  # if s%1000==0:
  #   print(f'Rolling 100 period average reward at {s} iterations is {avg_reward}')

  if s>1 and s%eval_freq==0:
    print(s)
    evaluations.append(evaluate_policy(agent))
    
  s+=1



